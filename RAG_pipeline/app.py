import streamlit as st
import time
from dotenv import load_dotenv

# ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
from etl.loader import get_retriever
from rag.retriever import get_query_analyzer_chain, build_retriever_with_filters
from rag.chain import get_rag_chain

# 1. í™˜ê²½ ë³€ìˆ˜ (API KEY) ë¡œë“œ
load_dotenv()

# --- í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="í•œ-ë¯¸ í†µí•© ê·œì • AI ë¹„ì„œ", page_icon="âœˆï¸", layout="centered")

st.title("âœˆï¸ ë˜‘ë˜‘í•œ ê³µí•­/ì„¸ê´€ AI ë¹„ì„œ")
st.markdown("""
ì´ ì±—ë´‡ì€ **í•œêµ­ì˜ ê³µê³µë°ì´í„°(í‘œ í˜•ì‹)**ì™€ **ë¯¸êµ­ CBP/TSA ê·œì •(Q&A í˜•ì‹)**ì´  
í•˜ë‚˜ì˜ í†µì¼ëœ ìŠ¤í‚¤ë§ˆë¡œ ì •ê·œí™”(ETL)ëœ RAG íŒŒì´í”„ë¼ì¸ ìœ„ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤.
""")

# --- ì„¸ì…˜ ì´ˆê¸°í™” (ëŒ€í™” ê¸°ë¡, ì—”ì§„ ë“±) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# ìºì‹±ì„ ì´ìš©í•´ RAG ê´€ë ¨ ë¬´ê±°ìš´ ì²´ì¸ë“¤ì„ ë”± í•œ ë²ˆë§Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
@st.cache_resource
def init_rag_components():
    # 1. ë°ì´í„°ë² ì´ìŠ¤(Chroma) ì—°ê²°
    base_retriever = get_retriever()
    # 2. ì§ˆë¬¸ ë¶„ì„ê¸°
    query_analyzer = get_query_analyzer_chain()
    # 3. ë‹µë³€ ìƒì„± ì²´ì¸
    qa_chain = get_rag_chain()
    
    return base_retriever, query_analyzer, qa_chain

try:
    base_retriever, query_analyzer, qa_chain = init_rag_components()
except Exception as e:
    st.error("RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. API í‚¤ë‚˜ DB ê²½ë¡œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- UI: ê¸°ì¡´ ì±„íŒ… ê¸°ë¡ í™”ë©´ì— ë¿Œë¦¬ê¸° ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ë©”ì¸ ì±„íŒ… ë¡œì§ ---
# ì‚¬ìš©ìê°€ ì±„íŒ…ì°½ì— ì…ë ¥ì„ ë„£ìœ¼ë©´ ì‘ë™í•©ë‹ˆë‹¤
if prompt := st.chat_input("ì˜ˆ: ì»µë¼ë©´(ìœ¡ë¥˜ìŠ¤í”„ ë‚´ì¥) ë¯¸êµ­ ê°ˆ ë•Œ ìœ„íƒìœ¼ë¡œ ë¶€ì³ë„ ë˜ë‚˜ìš”?"):
    
    # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•˜ê³  í™”ë©´ì— ì¦‰ì‹œ ë„ì›€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. AI ë´‡ ë‹µë³€ ìƒì„± ì˜ì—­
    with st.chat_message("assistant"):
        # UI ì‘ë‹µì„±ì— ë„ì›€ì„ ì£¼ëŠ” "ìƒíƒœ ì°½" í‘œì‹œ
        with st.status("ğŸ” ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ê·œì •ì„ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤...", expanded=True) as status:
            
            # Step A: ì˜ë„ ì¶”ì¶œ
            st.write("1ï¸âƒ£ ì§ˆë¬¸ ì˜ë„ íŒŒì•… ì¤‘...")
            intent = query_analyzer.invoke({"query": prompt})
            st.write(f"ğŸ“ *íŒŒì•…ëœ ì˜ë„: ëª©ì ì§€='{intent.target_country}', ìˆ˜í•˜ë¬¼í˜•íƒœ='{intent.transport_method}', í•µì‹¬í‚¤ì›Œë“œ='{intent.item_name}'*")
            
            # Step B: í•„í„° ë°˜ì˜ëœ ê²€ìƒ‰ ìˆ˜í–‰
            st.write("2ï¸âƒ£ í•„í„°ê°€ ê²°í•©ëœ Vector DB ê²€ìƒ‰ ì¤‘...")
            # ì‹¤ì œ DBì—ì„œëŠ” í•„í„°ë¥¼ ê²°í•©í•œ ê²€ìƒ‰ê¸°ë¥¼ ìƒˆë¡œ ë§Œë“¦.
            vectorstore = base_retriever.vectorstore
            filtered_retriever = build_retriever_with_filters(vectorstore, intent)
            
            # ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê¸ì–´ì˜µë‹ˆë‹¤
            docs = filtered_retriever.invoke(prompt)
            
            if not docs:
                status.update(label="âš ï¸ ê´€ë ¨ëœ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", state="error")
                st.stop()
            
            # Context ë¬¸ìì—´ ì¡°í•©
            context_text = "\n\n".join([f"[ì¶œì²˜: {d.metadata.get('source', 'ì•Œìˆ˜ì—†ìŒ')}]\n{d.page_content}" for d in docs])
            
            # UI ì‹œê°ì  ì¦ëª… (ì •ê·œí™”ëœ ë©”íƒ€ë°ì´í„°)
            st.write("3ï¸âƒ£ ë°œê²¬ëœ ë‹¨ì¼ ìŠ¤í‚¤ë§ˆ ê·œì •(í•œ/ë¯¸ í˜¼í•©):")
            for i, d in enumerate(docs, 1):
                st.caption(
                    f"ğŸ’¡ ë¬¸ì„œ {i}: {d.metadata.get('category')} | ì œí•œ: ê¸°ë‚´({d.metadata.get('carry_on')}), "
                    f"ìœ„íƒ({d.metadata.get('checked_baggage')}), ë¯¸êµ­ì…êµ­({d.metadata.get('us_customs_admissibility')})"
                )
            
            status.update(label="âœ… ê²€ìƒ‰ ì™„ë£Œ! ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.", state="complete", expanded=False)

        # Step C: LLM ìµœì¢… ë‹µë³€ ìƒì„±(ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
        message_placeholder = st.empty()
        full_response = ""
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹(ê¸€ìê°€ í•˜ë‚˜ì”© ì³ì§€ëŠ” íš¨ê³¼)ìœ¼ë¡œ ë‹µë³€ ì œê³µ
        for chunk in qa_chain.stream({"context": context_text, "question": prompt}):
            full_response += chunk
            message_placeholder.markdown(full_response + "â–Œ")
        
        # ìµœì¢… ì™„ì„±ëœ ë‹µë³€ ì¶œë ¥
        message_placeholder.markdown(full_response)
        
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ë´‡ì˜ ë‹µë³€ì„ ì„¸ì…˜ì— ì €ì¥ (ë‹¤ìŒ ëŒ€í™” íë¦„ì„ ìœ„í•´)
        st.session_state.messages.append({"role": "assistant", "content": full_response})
