from dotenv import load_dotenv

from etl.extractor import extract_raw_data
from etl.transformer import build_transformer_chain, process_chunk_to_document
from etl.loader import build_vector_db

def run_etl_pipeline():
    # 1. í™˜ê²½ ë³€ìˆ˜(API í‚¤ ë“±) ë¡œë“œ
    load_dotenv()
    
    print("============================================================")
    print("  ğŸš€ 1ë‹¨ê³„: Extractor (ë°ì´í„° ì¶”ì¶œ)")
    print("============================================================")
    # í˜„ì¬ëŠ” í•˜ë“œì½”ë”©ëœ í•œêµ­ì–´/ì˜ì–´ ìŠ¤ë‹ˆí«ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    raw_chunks = extract_raw_data()
    for i, chunk in enumerate(raw_chunks, 1):
        print(f"ğŸ“„ Raw Chunk {i}: {chunk[:50]}...")
        
    print("\n============================================================")
    print("  ğŸ¤– 2ë‹¨ê³„: Transformer (gpt-5-mini ê¸°ë°˜ ê·œì • ì¶”ì¶œ & ë²ˆì—­)")
    print("============================================================")
    chain = build_transformer_chain()
    
    transformed_docs = []
    for i, chunk in enumerate(raw_chunks, 1):
        print(f"ğŸ”„ Processing Chunk {i}...")
        doc = process_chunk_to_document(chunk, chain)
        transformed_docs.append(doc)
        print(f"   [ê²°ê³¼] ë¬¼í’ˆëª…: {doc.metadata['item_name']} | ë¶„ë¥˜: {doc.metadata['category']} | ê²€ìƒ‰ìš© ë³¸ë¬¸ ê¸¸ì´: {len(doc.page_content)}")
        print(f"   [ìƒì„¸ ì¡°ê±´(ë²ˆì—­ë¨)]: {doc.page_content.split('ê·œì • ìƒì„¸: ')[-1][:50]}...")
        
    print("\n============================================================")
    print("  ğŸ—„ï¸ 3ë‹¨ê³„: Loader (Chroma DB ì ì¬ ë° ë‹¤êµ­ì–´ ì„ë² ë”©)")
    print("============================================================")
    # Chroma DBì— documentë“¤ì„ ì ì¬í•©ë‹ˆë‹¤.
    vector_db = build_vector_db(transformed_docs)
    
    print("\nğŸ‰ ETL íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ êµ¬ì¶• ë° ì ì¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # [Self-Querying ì‹œë®¬ë ˆì´ì…˜]
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Self-Query Retrieverê°€ ì´ í•„í„°ë¥¼ LLMì„ í†µí•´ ìë™ ìƒì„±í•©ë‹ˆë‹¤.
    print("\nğŸ” (í…ŒìŠ¤íŠ¸) í•„í„°ê°€ ì ìš©ëœ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜: ìœ„íƒ ìˆ˜í•˜ë¬¼ì´ ê¸ˆì§€ëœ ì „ìê¸°ê¸° ê²€ìƒ‰")
    test_retriever = vector_db.as_retriever(
        search_kwargs={
            "k": 1,
            "filter": {
                "$and": [
                    {"category": "ì „ìê¸°ê¸°"},
                    {"checked_baggage": "ê¸ˆì§€"}
                ]
            }
        }
    )
    results = test_retriever.invoke("ê¸°ë‚´ì— ê°€ì ¸ê°€ë„ ë˜ëŠ” ë³´ì¡° ë°°í„°ë¦¬")
    if results:
        print(f"   -> ê²€ìƒ‰ëœ ë¬¸ì„œ: {results[0].metadata['item_name']} (ì¶œì²˜: {results[0].metadata['source']})")
    else:
        print("   -> ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_etl_pipeline()
