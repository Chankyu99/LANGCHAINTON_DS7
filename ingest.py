"""
ingest.py
---------
data/index_docstore_export.jsonl íŒŒì¼ì„ ì½ì–´
ChromaDBì— ì„ë² ë”©ì„ ì €ì¥í•©ë‹ˆë‹¤.

ì‹¤í–‰:
    .venv/bin/python ingest.py
"""

import json
import os
from pathlib import Path

from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

# â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

# â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR   = Path(__file__).parent
DATA_FILE_ORIGINAL = BASE_DIR / "data" / "index_docstore_export.jsonl"
DATA_FILE_AUGMENTED = BASE_DIR / "data" / "index_docstore_augmented.jsonl"
CHROMA_DIR = BASE_DIR / "chroma_db"
COLLECTION_NAME = "airline_regulations"


def get_data_file() -> Path:
    """ì¡´ì¬í•˜ëŠ” ë°ì´í„° íŒŒì¼ì„ ê°€ì ¸ì˜¨ë‹¤. augmented íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ìš°ì„  ì ìš©í•œë‹¤."""
    if DATA_FILE_AUGMENTED.exists():
        print(f"âœ¨ ì¦ê°•ëœ(Augmented) ë°ì´í„° íŒŒì¼ì„ ë°œê²¬í•˜ì—¬ ìš°ì„  ì ìš©í•©ë‹ˆë‹¤: {DATA_FILE_AUGMENTED.name}")
        return DATA_FILE_AUGMENTED
    return DATA_FILE_ORIGINAL

def load_jsonl(filepath: Path) -> list[dict]:
    """JSONL íŒŒì¼ì„ ì½ì–´ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜."""
    records = []
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                records.append(json.loads(line))
    return records


def build_documents(records: list[dict]) -> list[Document]:
    """
    JSONL ë ˆì½”ë“œ â†’ LangChain Document ë³€í™˜.
    recommended_metadata ê°’ì„ ë©”íƒ€ë°ì´í„°ë¡œ ë§¤í•‘.
    ChromaDBëŠ” ë©”íƒ€ë°ì´í„° ê°’ìœ¼ë¡œ str/int/float/boolë§Œ í—ˆìš©í•˜ë¯€ë¡œ
    ì¤‘ì²© dictëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜.
    """
    docs = []
    for rec in records:
        meta_raw = rec.get("recommended_metadata", {})
        # ChromaDB í˜¸í™˜: ëª¨ë“  ê°’ì„ str ì²˜ë¦¬
        metadata = {k: str(v) for k, v in meta_raw.items()}
        # doc_id ë° country ì¶”ê°€
        metadata["doc_id"]  = rec.get("doc_id", "")
        metadata["country"] = rec.get("country", "")

        docs.append(Document(
            page_content=rec["page_content"],
            metadata=metadata,
        ))
    return docs


def main():
    print("=" * 50)
    print("ğŸ›« ê¸°ë‚´ë­ë¼ â€” ë°ì´í„° ì„ë² ë”© ì‹œì‘")
    print("=" * 50)

    # ì´ë¯¸ DBê°€ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
    if CHROMA_DIR.exists() and any(CHROMA_DIR.iterdir()):
        print(f"âœ… ChromaDBê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {CHROMA_DIR}")
        print("   ì¬ìƒì„±í•˜ë ¤ë©´ chroma_db/ í´ë”ë¥¼ ì‚­ì œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # 1) ë°ì´í„° ë¡œë“œ ì°¾ê¸°
    DATA_FILE = get_data_file()
    print(f"\nğŸ“‚ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {DATA_FILE}")
    records = load_jsonl(DATA_FILE)
    print(f"   â†’ {len(records)}ê°œ ë ˆì½”ë“œ ë°œê²¬")

    # 2) Document ë³€í™˜
    docs = build_documents(records)
    print(f"   â†’ {len(docs)}ê°œ ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ")

    # 3) ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    print("\nğŸ”‘ OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ (text-embedding-3-small)...")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # 4) ChromaDB ì €ì¥
    print(f"\nğŸ’¾ ChromaDB ì €ì¥ ì¤‘: {CHROMA_DIR}")
    vectorstore = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        collection_name=COLLECTION_NAME,
        persist_directory=str(CHROMA_DIR),
    )

    print(f"\nâœ… ì„ë² ë”© ì™„ë£Œ! ì´ {len(docs)}ê°œ ë¬¸ì„œê°€ chroma_db/ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 50)


if __name__ == "__main__":
    main()
